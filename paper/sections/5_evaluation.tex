%!TEX root = ../paper.tex

\section{Evaluation}
\label{sec:evaluation}

\todo{Write something here}

\subsection{Evaluation measures}
\label{sub:evaluation_measures}
We will use the following measures to evaluate our approach:
as already mentioned in a previous section, the demand classifier can be optimized for either precision or recall, depending on the use case.
Therefore, we consider three values:
\begin{itemize}
	\item
		\emph{Precision of demand posts} \todo{formula}
	\item
		\emph{Recall of demand posts} \todo{formula}
	\item
		\emph{Overall accuracy} \todo{formula}
\end{itemize}
This captures the most important aspects of our system: If we predict a demand, how likely is it really a demand post, and how many of all demand posts can we actually find.
Finally, the overall accuracy is usually quite high because of the data skew in the demand tagging.
However, we left this in for the sake of completeness.

Since there is no such data skew in the product classifier, we just consider the overall precision, i.e. what percentage of our recommendations were correct.


\subsection{Results}
\label{sub:results}

Compared with a naive approach, using the brochures as they are, the translation of the German brochures and the enlargement of the training brings an improvement of ...\% for the demand classifier as it is shown in the table \ref{table:demand_trainset_enlargement}.
\paragraph{Decisn decision 1}
\todo{Graph or table}
\paragraph{Decisn decision 2}
\todo{Graph or table}
\paragraph{Decisn decision 3}
\todo{Graph or table}

\endinput
\begin{itemize}
	\item Introduce our data set (show numbers, show examples)
	\item How we got our training data (everything at least twice, use the demands for learning, use the products for evaluation, Active Learning approach)
	\item Reference Precision/Recall from above again, final
	\item Evaluate of the training data generation, which approach is best (random, grouping)
\end{itemize}


