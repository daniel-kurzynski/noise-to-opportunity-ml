%!TEX root = ../paper.tex

\section{Evaluation}
\label{sec:evaluation}

The following \todo{}

\subsection{Evaluation measures}
\label{sub:evaluation_measures}
We will use the following measures to evaluate our approach:
as already mentioned in a previous section, the demand classifier can be optimized for either precision or recall, depending on the use case.
Therefore, we consider three values:
\begin{itemize}
	\item
		\emph{Precision of demand posts}
		$P_{demand} = \frac{correct~predicted~demands}{predicted~demands}$
	\item
		\emph{Recall of demand posts}
		$R_{demand} = \frac{correct~predicted~demands}{all~demand~posts}$
	\item
		\emph{Overall precision}
		$P_{all} = \frac{correct~predictions}{all~predictions}$
\end{itemize}
This measures capture the most important aspects of our system: If we predict a demand, how likely is it really a demand post, and how many of all demand posts can we actually find.
Finally, the overall precision is usually quite high because of the data skew in the demand tagging.
A tagger, which always returns ``no-demand'' will have an overall precision of \todo{percent}, as seen in Table~\todo{NUMBER}.
However, we left this in for the sake of completeness.

Since there is no such data skew in the product classifier, we just consider the overall accuracy, i.e. what percentage of our predictions were correct.


\subsection{Results}
\label{sub:results}

\subsubsection{Demand classifier}
\label{ssub:demand_classifier}


\subsubsection{Product classifier}
\label{ssub:product_classifier}


\subsubsection{Design decision 1: feature selection}

\subsubsection{Design decision 2: splitting size}

\subsubsection{Design decision 3: translation and book descriptions}

Compared with a naive approach, using the brochures as they are, the translation of the German brochures and the enlargement of the training brings an improvement of ...\% for the demand classifier as it is shown in the table \ref{table:demand_trainset_enlargement}.

\endinput
\begin{itemize}
	\item Introduce our data set (show numbers, show examples)
	\item How we got our training data (everything at least twice, use the demands for learning, use the products for evaluation, Active Learning approach)
	\item Reference Precision/Recall from above again, final
	\item Evaluate of the training data generation, which approach is best (random, grouping)
\end{itemize}


