%!TEX root = ../paper.tex

\section{Evaluation}
\label{sec:evaluation}

The following sections show the evaluation of our approach.
We start with defining relevant evaluation measures and then evaluate our two classifiers and relevant design decisions.

\subsection{Evaluation measures}
\label{sub:evaluation_measures}
We will use the following measures to evaluate our approach:
As already mentioned in a previous section, the demand classifier can be optimized for either precision or recall, depending on the use case.
Additionally, we look at the overall precision of both, the demand and product classifiers.
Therefore, we consider three values:

\begin{align*}
	\emph{Precision of the demand classifier}: P_{demand} 			&= \frac{correct~predicted~demands}{predicted~demands} \\
	\emph{Recall of the demand classifier}: R_{demand} 				&= \frac{correct~predicted~demands}{all~demand~posts} \\
	\emph{Overall precision of the demand and product classifier}: P_{all} &= \frac{correct~predictions}{all~predictions} \\
\end{align*}

This measures capture the most important aspects of our system: If we predict a demand, how likely is it really a demand post ($P_{demand}$), and how many of all demand posts can we actually find ($R_{demand}$).
Finally, the overall precision ($P_{all}$) of the demand classifier is usually quite high because of the data skew in the demand tagging.
A tagger, which always returns ``no-demand'' will have an overall precision of \todo{percent}, as seen in Table~\todo{NUMBER}.
However, we left this in for the sake of completeness and take all three measure in count to evaluate demand classifier performance.
Since there is no such data skew in the product classifier, the overall precision is sufficient to describe the product classifier performance.


\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\textwidth]{figures/product_eval.eps}
	\end{center}
	\caption{Comparison of different classifiers and different classification modes}
	\label{fig:product_eval}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_feature_selection_with_none.eps}
		\caption{with ``no-product'' prediction}
	\end{subfigure}~
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_feature_selection_without_none.eps}
		\caption{without ``no-product'' prediction}
	\end{subfigure}
	\caption{Feature Selection: comparison of ten-, 100- and 1000-most occurred words selected as tf-idf features}
	\label{fig:product_feature_selection}
\end{figure}

\todo{Find better descriptions for data}
\todo{Avoid glich of values in bars}

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_translate_amazon_with_none.eps}
		\caption{with ``no-product'' prediction}
	\end{subfigure}~
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_translate_amazon_without_none.eps}
		\caption{without ``no-product'' prediction}
	\end{subfigure}
	\caption{Feature Selection: comparison of ten-, 100- and 1000-most occurred words selected as tf-idf features}
	\label{fig:product_translate_amazon}
\end{figure}

\subsection{Results}
\label{sub:results}

\subsubsection{Demand classifier}
\label{ssub:demand_classifier}

\begin{table}[h]
	\centering
	\begin{tabular}{lc}
		\hline
		\textbf{Metric} & \textbf{Result}  \\
		\hline
		\hline
		Precision of the demand classifier & 88~\% \\
		\hline
		Recall of the demand classifier & 51~\%  \\
		\hline
		Overall precision of the demand classifier & 93~\%  \\
		\hline
	\end{tabular}
	\caption{Cross validation results on demand classifier}
	\label{table:demand_evaluation}
\end{table}

\todo{WRITE HERE SOMETHING!}


\subsubsection{Product classifier}
\label{ssub:product_classifier}

As we said before, linear classifiers performed best for the product classification.
Because they only have a small difference, we will evaluate on three linear classifiers: logistic regression, perceptron, and support vector machines.
Comparing these classifiers with a base approach (K-Nearest-Neighbor classification) we can see a better performance as it is shown in figure \ref{fig:product_eval}.
In this figure you can see the four classifiers, which are predicting for each post exactly one of the four products.
Additionally to this ``forced'' prediction (the without ``no-product'' prediction) we have modified the linear classifiers in the way, that they predict for each post the four product if and only if one of the product predictions has a significantly greater probability.
In the other case, so if all products are predicted with quite equal probability, ``no-product'' is predicted as the product.
This classification mode is nearer to the real world scenario, since not every LinkedIn post we have crawled can be mapped to one of the product classes.
And as the figure \ref{fig:product_eval} shows, this approach increases the accuracy by about seven to ten percent.

For the final product classifier we have made some improvements, which are based on some design decisions.
We will introduce and evaluate two of these design decisions in the following subsections.

\subsubsection{Design decision 1: feature selection}
As already mentioned in the section \ref{sub:feature-selection}, we use for the product classifier the classical tf-idf feature selection.
Additionally we pick only the ten words with the highest tf-idf assuming, that these words describe the product best.
The comparison of the ten-best approach with the 100-best and 1000-best approaches you can see in the figure \ref{fig:product_feature_selection}
As you can see, both classification modes (with and without ``no-demand'' prediction), the ten-best feature selection performs best.

\subsubsection{Design decision 2: translation and book descriptions}
Section \ref{sub:initial_data_set} described our approach for enlargement and linguistic adaptation of the training set.
Compared with a naive approach, using the brochures as they are, the translation of the German brochures and the enlargement of the training brings an improvement of 17-20\% (for without-no-product mode) and 16-28\% (for the with-no-product mode) for the demand classifier as it is shown in the figure \ref{fig:product_translate_amazon}.
We can also see, that each of the steps, the translation and the enlargement, improves the performance.
So having same problems with other data sets, we assume that this design decision could improve any other classifier implementation in the same manner.

\subsection{Two stage classifier}
\label{sub:two_stage_classifier}

As we propose in our work to use a two stage classifier for the \nto problem.
To evaluate this approach we compare our two-stage classifier against just a single classifier using the brochures and LinkedIn posts as training data.
% This one-stage classifier is identical to our product classifier.

For that evaluation we change the test set annotations in the way, that if an annotated (our manual annotation) post was been classified as ``no-demand'', the product classification will be ignored, so it will become ``no-product''.
Thus we can compare the predictions of the product classifier (which are one of the products or ``no-product'') with the predictions of the two stage classifier (which are ``demand'' or ``no-demand'' and if ``demand'' then one of the products or ``no-product'').
Because both classifiers are trained on the LinkedIn posts, we do a cross validation on the LinkedIn posts.
Otherwise, we would learn and test on the same data.
We use a ten fold cross validation here.
The accuracy of the both classification you can see in Table \ref{table:two_stage_eval}, which shows that the two stage approach performs better than the single classifier approach.

The better performance confirms our concept that the demand and product recognition are two different things, and should be learned and tuned separately.

\begin{table}
	\centering
	\begin{tabular}{c|c}
		\hline
		Product classification & Two stage classification \\ \hline \hline
		69.73\% & 81.35\% \\ \hline
	\end{tabular}
	\caption{Comparison of the standalone product classification with the two stage classification}
	\label{table:two_stage_eval}
\end{table}
