%!TEX root = ../paper.tex

\section{Evaluation}
\label{sec:evaluation}

The following sections show the evaluation of our approach.
We start with defining relevant evaluation measures and then evaluate our two classifiers and relevant design decisions.

\subsection{Evaluation measures}
\label{sub:evaluation_measures}
We will use the following measures to evaluate our approach:
As already mentioned in a previous section, the demand classifier can be optimized for either precision or recall, depending on the use case.
Therefore, we consider \todo{three or four} values:

\begin{align*}
	\emph{Precision of the demand classifier}: P_{demand} 			&= \frac{correct~predicted~demands}{predicted~demands} \\
	\emph{Recall of the demand classifier}: R_{demand} 				&= \frac{correct~predicted~demands}{all~demand~posts} \\
	\emph{Overall precision of the demand classifier}: P_{all} &= \frac{correct~predictions}{all~predictions} \\
	\emph{Accuracy of product classifier}: A_{all} 						&= \frac{correct~predictions}{all~predictions} \\
\end{align*}

This measures capture the most important aspects of our system: If we predict a demand, how likely is it really a demand post ($P_{demand}$), and how many of all demand posts can we actually find ($R_{demand}$).
Finally, the overall precision ($P_{all}$) is usually quite high because of the data skew in the demand tagging.
A tagger, which always returns ``no-demand'' will have an overall precision of \todo{percent}, as seen in Table~\todo{NUMBER}.
However, we left this in for the sake of completeness.

Since there is no such data skew in the product classifier, we just consider the overall accuracy($A_{all}$), i.e. what percentage of our predictions were correctly classified.\todo{Accuracy and overall precision are the same?}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\textwidth]{figures/product_eval.eps}
	\end{center}
	\caption{Comparison of different classifiers and different classification modes}
	\label{fig:product_eval}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_feature_selection_with_none.eps}
		\caption{with ``no-product'' prediction}
	\end{subfigure}~
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_feature_selection_without_none.eps}
		\caption{without ``no-product'' prediction}
	\end{subfigure}
	\caption{Feature Selection: comparison of ten-, 100- and 1000-most occurred words selected as tf-idf features}
	\label{fig:product_feature_selection}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_translate_amazon_with_none.eps}
		\caption{with ``no-product'' prediction}
	\end{subfigure}~
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_translate_amazon_without_none.eps}
		\caption{without ``no-product'' prediction}
	\end{subfigure}
	\caption{Feature Selection: comparison of ten-, 100- and 1000-most occurred words selected as tf-idf features}
	\label{fig:product_translate_amazon}
\end{figure}

\subsection{Results}
\label{sub:results}

\subsubsection{Demand classifier}
\label{ssub:demand_classifier}
\todo{WRITE HERE SOMETHING!}


\subsubsection{Product classifier}
\label{ssub:product_classifier}

As we said before, linear classifiers performed best for the product classification.
Because they only have a small difference, we will evaluate on three linear classifiers: logistic regression, perceptron, and support vector machines.
Comparing these classifiers with a naive approach (K-Nearest-Neighbor classification) we can see a better performance as it is shown in figure \ref{fig:product_eval}.
In this figure you can see the four classifiers, which are predicting for each post exactly one of the four products.
Additionally to this ``forced'' prediction (the without ``no-product'' prediction) we have modified the linear classifiers in the way, that they predict for each post the four product if and only if one of the product predictions has a significantly greater probability.
In the other case, so if all products are predicted with quite equal probability, ``no-product'' is predicted as the product.
This classification mode is nearer to the real world scenario, since not every LinkedIn post we have crawled can be mapped to one of the product classes.
And as the figure \ref{fig:product_eval} shows, this approach increases the accuracy by about seven to ten percent.

For the final product classifier we have made some improvements, which are based on some design decisions.
We will introduce and evaluate two of these design decisions in the following subsections.

\subsubsection{Design decision 1: feature selection}
As already mentioned in the section \ref{sub:feature-selection}, we use for the product classifier the classical tf-idf feature selection.
Additionally we pick only the ten words with the highest tf-idf assuming, that these words describe the product best.
The comparison of the ten-best approach with the 100-best and 1000-best approaches you can see in the figure \ref{fig:product_feature_selection}
As you can see, both classification modes (with and without ``no-demand'' prediction), the ten-best feature selection performs best.

\subsubsection{Design decision 2: translation and book descriptions}
Section \ref{sub:initial_data_set} described our approach for enlargement and linguistic adaptation of the training set.
Compared with a naive approach, using the brochures as they are, the translation of the German brochures and the enlargement of the training brings an improvement of 17-20\% (for without-no-product mode) and 16-28\% (for the with-no-product mode) for the demand classifier as it is shown in the figure \ref{fig:product_translate_amazon}.
We can also see, that each of the steps, the translation and the enlargement, improves the performance.
So having same problems with other data sets, we assume that this design decision could improve any other classifier implementation in the same manner.

\subsection{Two stage classifier}
\label{sub:two_stage_classifier}

As we propose in our work to use a two stage classifier for the \nto problem, we have also compared the performance of the product classifier alone against a combination of the demand and the product classifier.
For that evaluation we have changed the test set annotations in the way, that if an annotated (our manual annotation) post was been classified as ``no-demand'', the product classification will be ignored, so it will be ``no-product''.
Thus we can compare the predictions of the product classifier (which are one of the products or ``no-product'') with the predictions of the two stage classifier (which are ``demand'' or ``no-demand'' and if ``demand'' then one of the products or ``no-product'').
Because the demand classifier is also trained on the LinkedIn posts, we have to do a cross validation on the LinkedIn posts, because otherwise, we would learn and train on the same data.
We have used a ten fold cross validation here.
The accuracy of the both classification you can see in Table \ref{table:two_stage_eval}.
As you can see, the two stage approach performs better than the single product classification.

\begin{table}
	\centering
	\begin{tabular}{c|c}
		\hline
		Product classification & Two stage classification \\ \hline \hline
		38.92\% & 81.35\% \\ \hline
	\end{tabular}
	\caption{Comparison of the standalone product classification with the two stage classification}
	\label{table:two_stage_eval}
\end{table}

\todo{noch weiter ausführen, was das ergbniss nun heißt? und warum wir so gut sind?}

