%!TEX root = ../paper.tex

\section{Evaluation}
\label{sec:evaluation}

\todo{Avoid glich of values in bars}

The following sections show the evaluation of our approach.
We start with defining our evaluation measures and then evaluate our two classifiers and relevant design decisions.

\subsection{Evaluation measures}
\label{sub:evaluation_measures}
We will use the following measures to evaluate our approach:
As already mentioned in a previous section, the demand classifier can be optimized for either precision or recall, depending on the use case.
Therefore, we consider three values:

\begin{align*}
	\emph{Precision of the demand classifier}~P_{demand} 			&= \frac{correct~predicted~demands}{predicted~demands} \\
	\emph{Recall of the demand classifier}~R_{demand} 				&= \frac{correct~predicted~demands}{all~demand~posts} \\
	\emph{Overall precision for both classes}~P_{all} &= \frac{correct~predictions}{all~predictions} \\
\end{align*}

This measures capture the most important aspects of our system: If we predict a demand, how likely is it really a demand post ($P_{demand}$), and how many of all demand posts can we actually find ($R_{demand}$).
Finally, the overall precision ($P_{all}$) of the demand classifier is usually quite high because of the data skew in the demand tagging.

For example, a tagger, which always returns ``no-demand'' will have a relatively high overall precision of $\frac{406}{445}$ = $91.24~\%$, as seen in Table~\ref{table:data_overview}.
However, we left this in for the sake of completeness and take all three measure in count to evaluate demand classifier performance.
Since there is no such data skew in the product classifier, the overall precision is sufficient to describe the product classifier performance.

\subsection{Results}
\label{sub:results}

\subsubsection{Demand classifier}
\label{ssub:demand_classifier}

We optimized our demand classifier with respect to the demand precision.
Table~\ref{table:demand_evaluation} shows that we classify 88~\% of the posts, which express a demand, correctly.
We measured this number by doing a cross validation on the tagged posts.
Cross validations means, that we split the data set in ten parts, then run the feature extraction and the learning phase on $9 / 10$ of the data to predict one the remaining $1 / 10$.
This is done repeatedly for all ten parts.
By optimizing for precision we make sure to decrease the work load of the salesmen: If we propose a post, we are quite sure, that the users needs something.

\begin{table}[h]
	\centering
	\begin{tabular}{lc}
		\hline
		\textbf{Metric} & \textbf{Result}  \\
		\hline
		\hline
		$P_{demand}$ & 88~\% \\
		\hline
		$R_{demand}$ & 51~\%  \\
		\hline
		$P_{all}$ & 93~\%  \\
		\hline
	\end{tabular}
	\caption{Cross validation results on demand classifier}
	\label{table:demand_evaluation}
\end{table}

\subsubsection{Product classifier}
\label{ssub:product_classifier}

As we said before, linear classifiers performed best for the product classification.
Because they only have a small difference, we will evaluate on three linear classifiers: Logistic Regression, Perceptron, and Support Vector Machines.
We compare these classifiers with a baseline approach (k-nearest-neighbor classification), see Figure~\ref{fig:product_eval}.
In this figure you can see the four classifiers, which are predicting exactly one of the four products for each post.
We evaluated two different cases:
first, we only predicted on posts, which we tagged as belonging to one of the four products.
In the second case, we evaluated on all posts, so that the classifier sometimes has to predict ``None''.
Of course this value is lower than the first one, because the classifier can now also wrongly predict the ``None'' class.
This case is nearer to the real world scenario, since not every LinkedIn post we have crawled can be mapped to one of the product classes.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{figures/product_eval.eps}
	\end{center}
	\caption{Comparison of different classifiers and different classification modes}
	\label{fig:product_eval}
\end{figure}

In the following, we will take a closer look at some of the design decisions and evaluate their impact on the overall performance.

\subsubsection{Design decision 1: feature selection}
As already mentioned in the Section~\ref{sub:feature-selection}, we use tf-idf feature selection for the product classifier.
Figure~\ref{fig:product_feature_selection} shows the performance of the system for different numbers of features we select.
Additionally we pick only the ten words with the highest tf-idf assuming, that these words describe the product best.
The best performance was reached when picking only the ten words with the highest tf-idf values, which captures the intuition, that we have to be very strict to cope with the \emph{document mismatch problem}.
\begin{figure}
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_feature_selection_with_none.eps}
		\caption{with ``no-product'' prediction}
	\end{subfigure}~
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_feature_selection_without_none.eps}
		\caption{without ``no-product'' prediction}
	\end{subfigure}
	\caption{Comparison of 10-, 100- and 1000-most occuring words (with respect to their tf-idf values) selected as features.}
	\label{fig:product_feature_selection}
\end{figure}

\subsubsection{Design decision 2: translations and book descriptions}
Section \ref{sub:initial_data_set} described our approach for enlargement and linguistic adaptation of the training set.
Compared with a na\"ive approach, using the brochures as they are, the translation of the German brochures and the enlargement of the training brings an improvement of 17-20\% (for without-no-product mode) and 16-28\% (for the with-no-product mode) for the demand classifier as it is shown in the Figure \ref{fig:product_translate_amazon}.
We can also see, that each of the steps, the translation and the enlargement, improves the performance.
So having same problems with other data sets, we assume that this design decision could improve any other classifier implementation in the same manner.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_translate_amazon_with_none.eps}
		\caption{with ``no-product'' prediction}
	\end{subfigure}~
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{figures/product_translate_amazon_without_none.eps}
		\caption{without ``no-product'' prediction}
	\end{subfigure}
	\caption{Feature Selection: comparison of ten-, 100- and 1000-most occurring words selected as tf-idf features}
	\label{fig:product_translate_amazon}
\end{figure}

\subsubsection{Design decision 3: two-stage classifier}
\label{sub:two_stage_classifier}

As we propose in our work to use a two stage classifier for the \nto problem.
To evaluate this approach we compare our two-stage classifier against just a single classifier using the brochures and LinkedIn posts as training data.
% This one-stage classifier is identical to our product classifier.

For that evaluation we change the test set annotations in the way, that if an annotated (our manual annotation) post was been classified as ``no-demand'', the product classification will be ignored, so it will become ``no-product''.
Thus we can compare the predictions of the product classifier (which are one of the products or ``no-product'') with the predictions of the two stage classifier (which are ``demand'' or ``no-demand'' and if ``demand'' then one of the products or ``no-product'').
Because both classifiers are trained on the LinkedIn posts, we do a cross validation on the LinkedIn posts.
Otherwise, we would learn and test on the same data.
We use a ten fold cross validation here.
The accuracy of the both classification you can see in Table \ref{table:two_stage_eval}, which shows that the two stage approach performs better than the single classifier approach.

The better performance confirms our concept that the demand and product recognition are two different things, and should be learned and tuned separately.

\begin{table}
	\centering
	\begin{tabular}{c|c}
		\hline
		Product classification & Two stage classification \\ \hline \hline
		69.73\% & 81.35\% \\ \hline
	\end{tabular}
	\caption{Comparison of the standalone product classification with the two stage classification}
	\label{table:two_stage_eval}
\end{table}
