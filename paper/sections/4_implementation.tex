%!TEX root = ../paper.tex

\section{Implementation}
\label{sec:implementation}

\todo{
\begin{itemize}
	\item Do classifier optimization
	\item For the concrete data set, how do we split concretely.
\end{itemize}
}

\subsection{Example data set}
\label{sub:initial_data_set}
We implemented a prototype of our \nto system based on data from a Germany software company called SAP.
SAP is the biggest software vendor in Europe and builds software for both small and large enterprises.
SAP provided us with nearly 100 brochures about four SAP software products, which are explained in Table~\ref{table:products}.\todo{there are also texts from wikipedia in the brochures!}
We chose LinkedIn as the business-oriented social network, as it is the biggest and most popular platform.
We crawled approximately 19,000 posts from LinkedIn.

Unfortunately, around 75~\% of the brochures are in German and the LinkedIn posts are written in English.
To solve the language mismatch, we translated the documents with an machine translation tools and replaced the German brochures with its translations.
Additionally, we added some book descriptions from Amazon about these products to enlarge the training set.

\begin{table}[h]
	\centering
	\label{table:products}
	{
		\renewcommand\arraystretch{1.25}
		\begin{tabular}{lll}
			\hline
			\textbf{Product} & \multicolumn{2}{l}{\textbf{Explanation}} \\
			\hline\hline
			CRM  & \multicolumn{2}{p{10cm}}{\raggedright
				Customer relationship management software to manage a company's interactions with past, present and future, potential customers.} \\
			\hline
			ECOM  & \multicolumn{2}{p{10cm}}{\raggedright
				E commerce software helps to automatize sale processes in the context of the internet economy.} \\
			\hline
			HCM  & \multicolumn{2}{p{10cm}}{\raggedright
				Human capital management software to support and control personal processes.} \\
			\hline
			LVM  & \multicolumn{2}{p{10cm}}{\raggedright
				Landscape virtualization management software that helps to deploy and manage existing applications  in virtualized data centrers and cloud infrastructures.} \\
			\hline
		\end{tabular}
	}
	\caption{Different products in the training data set of SAP brochures.}
\end{table}

\subsection{Manual annotation of the LinkedIn posts}

At the moment, there exists no data set for a demand classification, which can be used for learning and evaluation purposes.
Therefore, we tagged some post manually.
We used an active learning \nr approach to create the gold standard for both training and evaluation:
first, we randomly tagged some posts, then we built a basic classifier, which we ran over all posts to finally ask for a tagging decision for the most unsure post.
Then we reran the classification and the classifier asked again.
Using this approach, we are able to get the most out of our limited tagging time.

We built a tagging web app which implements the active learning.
All together, we tagged about 350 LinkedIn posts, both in respect of demand and product.
In an initial step, each post was tagged by at least two different persons to avoid misclassifications because of the opinion of one tagger.
If there were conflicts between the tagging decisions, a third person was tagging again to solve the conflict.
To evaluate the demand classifier, we use a ten-fold cross validation of all posts, because we need the posts also for training.
To evaluate the product classifier, we can use all annotated posts, because the learning works on the brochures.

\subsection{Demand Classifier}
For the demand classifier, we added the following manually built features, which have been show to increase the overall performance:
\begin{itemize}
	\item Number of questions in the post. Demand posts usually have more questions.
	\item Number of imperatives in the post. Demand posts usually have more imperatives like ``Please help me out" or ``Give me advice".
	\item Whether an e-mail address was given in the post. Demand posts often give an e-mail address to send answers to.
\end{itemize}

We decided on a na\"{i}ve bayes classifier with a Bernoulli event model.
The bernoulli event model is especially suited for \todo{write on}.

\subsection{Product Classifier}
To be done.
\todo{Introduce normalization and binary features.}

\subsection{Sampling algorithm}
As already mentioned in the previous sections, we extend our training set further by splitting the brochures and Amazon product texts into smaller chunks.
We end up with 994 documents, which we use for the training of the product classifier.

How we split.
