%!TEX root = ../paper.tex

\section{Evaluation}
\label{sec:evaluation}

\todo{Write something here}

\subsection{Evaluation data set} % (fold)
\label{sub:initial_data_set}
We built a prototype of an \nto system.
This prototype is build for the products of a German software company called SAP.
SAP is the biggest software vendor in Europe and builds software for both small and large enterprises.
SAP provided us with nearly 100 brochures about four SAP software products, which are explained in Table~\ref{table:products}.
We chose LinkedIn as the business-oriented social network, as it is the biggest and most popular platform.
We crawled approximately 19,000 LinkedIn posts.
Unfortunately three forth of the brochures are in German language and the LinkedIn posts are written in English.
To solve the language mismatch, we have translated the documents with an automated translator and replaced the German brochures with its translations.
Additionally we have added some book descriptions from Amazon about these products to enlarge the training set.
As already mentioned in the previous sections to extend our training set further, we split the brochures and the Amazon descriptions.
With the help of the splitting the training set grows to 994 document, which we use for the training of the product classifier.

% subsection initial_data_set (end)

\subsection{Manual annotation of the LinkedIn posts} % (fold)
\label{sub:manual_annotation_of_the_linkedin_posts}

At the moment, there exists no data set for a demand classification, which can be used for learning and evaluation purposes: we tagged some post manually.
We used an active learning \nr approach to create the gold standard for both training and evaluation:
first, we randomly tagged some posts, then we build a basic classifier, which repeatedly asked for those instances, where it was most unsure about.
Then it reran the classification and asked again.
We have implemented a tagging web app which implements the active learning.
All together, we tagged about 350 LinkedIn posts, both for demand and product.
In an initial step, each post was tagged at least twice to avoid misclassifications because of the opinion of one tagger.
If there were conflicts between the tagging decisions, a third person was tagging again to solve the conflict.
To evaluate the demand classifier, we use a ten-fold cross validation.
Since we have brochures for the products, on which we can learn, we used all annotated posts exclusively for the evaluation in the case of the product classifier.

% subsection manual_annotation_of_the_linkedin_posts (end)

\subsection{Evaluation measures}
\label{sub:evaluation_measures}
We will use the following measures to evaluate our approach:
as already mentioned in a previous section, the demand classifier can be optimized for either precision or recall, depending on the use case.
Therefore, we consider three values:
\begin{itemize}
	\item
		\emph{Precision of demand posts} \todo{formula}
		\emph{Recall of demand posts} \todo{formula}
		\emph{Overall accuracy}
\end{itemize}
This captures the most important aspects of our system: If we predict a demand, how likely is it really a demand post, and how many of all demand posts can we actually find.
Finally, the overall accuracy is usually quite high because of the data skew in the demand tagging.
However, we left this in for the sake of completeness.

Since there is no such data skew in the product classifier, we just consider the overall precision, i.e. what percentage of our recommendations were correct.


\subsection{Results} % (fold)
\label{sub:results}

Compared with a naive approach, using the brochures as they are, the translation of the German brochures and the enlargement of the training brings an improvement of ...\% for the demand classifier as it is shown in the table \ref{table:demand_trainset_enlargement}.
\paragraph{Decisn decision 1}
\todo{Graph or table}
\paragraph{Decisn decision 2}
\todo{Graph or table}
\paragraph{Decisn decision 3}
\todo{Graph or table}
% subsection results (end)

\endinput
\begin{itemize}
	\item Introduce our data set (show numbers, show examples)
	\item How we got our training data (everything at least twice, use the demands for learning, use the products for evaluation, Active Learning approach)
	\item Reference Precision/Recall from above again, final
	\item Evaluate of the training data generation, which approach is best (random, grouping)
\end{itemize}


